{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a92b5d0-897e-4ae4-8746-f3116f5ffece",
   "metadata": {},
   "source": [
    "## This notebook has the following topics -\n",
    "\n",
    "1. LLM models with langchain, getting started\n",
    "2. Async APIs for LLMs\n",
    "3. Custom LLM wrapper class\n",
    "4. Caching LLM Calls thorugh various design choices\n",
    "5. Serialize LLM classes\n",
    "6. Stream LLM and Chat model responses\n",
    "7. Quantifying the cost based on the token usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540ea6f2-4fd7-4dee-b504-b60067aee34d",
   "metadata": {},
   "source": [
    "## The Models\n",
    "\n",
    "1. **LLMs**\n",
    "2. ChatModels\n",
    "3. Text Embedding Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e919231-e76d-4f2a-b7b4-03c585d501d2",
   "metadata": {},
   "source": [
    "### LLMs  - getting started\n",
    "Large Language Models. It is a standard interface for LLM providers like OpenAI, HuggingFace etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "607ec367-06f1-4f64-997e-bdfe2b9e56da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side!'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name = \"text-ada-001\", n = 2, best_of = 2)\n",
    "\n",
    "llm(\"Tell me a joke with programing anecdote\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2a59956-c777-4c7f-b56c-b3eba4a99b50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# batch of requests -\n",
    "llm_result = llm.generate([\"Tell me a Scientific fact\"] * 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f17023d5-a604-4be7-b165-1edbb5ca0e39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[Generation(text='\\n\\nThe Earth is the only \"edient\" on the planet.', generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text='\\n\\nThe moon has an area of 1,000 square kilometers.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nThe moon has an\\n\\nassorted Slang words for these things meaning different things to different people\\n\\n1. its a place where people go to die\\n2. its a place where people can dream\\n3. its a place where people walk\\n4. its a place where peopleECE (electric chair)', generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text='\\n\\nThe universe is around one trillionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a\\n\\nThe universe is one trillionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of', generation_info={'finish_reason': 'length', 'logprobs': None})], [Generation(text=' about the universe\\n\\nThe universe is about the course of the universe and the existance of intelligent life.', generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text='\\n\\nThe human brain is about 10% the size of a human brain and has about a third the number of colors as a human brain.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nThe moon has an up-to-date on-base scientific team.', generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text='\\n\\nThe average life of a DNA molecule is 5.8 years.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nThe average life of a plant is five to seven years.', generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text='\\n\\nThe average lifespan of a human is about 78 years.', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'completion_tokens': 461, 'total_tokens': 486, 'prompt_tokens': 25}, 'model_name': 'text-ada-001'})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94408711-fdee-4ac9-9505-8b5c7f80de04",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(llm_result.generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4eecea06-c2d9-4175-9ed1-4f3375ac9540",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Generation(text='\\n\\nThe Earth is the only \"edient\" on the planet.', generation_info={'finish_reason': 'stop', 'logprobs': None}),\n",
       "  Generation(text='\\n\\nThe moon has an area of 1,000 square kilometers.', generation_info={'finish_reason': 'stop', 'logprobs': None})],\n",
       " [Generation(text='\\n\\nThe moon has an\\n\\nassorted Slang words for these things meaning different things to different people\\n\\n1. its a place where people go to die\\n2. its a place where people can dream\\n3. its a place where people walk\\n4. its a place where peopleECE (electric chair)', generation_info={'finish_reason': 'stop', 'logprobs': None}),\n",
       "  Generation(text='\\n\\nThe universe is around one trillionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a\\n\\nThe universe is one trillionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of', generation_info={'finish_reason': 'length', 'logprobs': None})]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_result.generations[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ee53177-799f-4d35-8d19-6563db8d3100",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'completion_tokens': 461,\n",
       "  'total_tokens': 486,\n",
       "  'prompt_tokens': 25},\n",
       " 'model_name': 'text-ada-001'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_result.llm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a108d816-7000-4a08-ac21-40b14a92af26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.get_num_tokens(\"what a joke\") # make sure you have installed tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b114bfe8-44ee-4dd4-95e8-78a01e6cdde6",
   "metadata": {},
   "source": [
    "### Async API for LLMs\n",
    "It uses asyncio library. This is useful for calling multiple LLMs concurrently, as these calls are network-bound.\n",
    "We can use ```agenerate``` method to call an OpenAI LLM asynchronously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d24fcbf8-5eae-4d75-a47c-e7934a476a95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import asyncio\n",
    "\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8609722b-ab4a-4bd4-bbe4-d75d6f7eedf6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_serially():\n",
    "    llm = OpenAI(temperature = 0.9)\n",
    "    for _ in range(10):\n",
    "        resp = llm.generate([\"Hello, how are you?\"])\n",
    "        print(resp.generations[0][0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0923dceb-9a3b-4c04-ab0d-6d9e1e0b1994",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "async def async_generate(llm):\n",
    "    resp = await llm.agenerate([\"Hello, how are you?\"])\n",
    "    print(resp.generations[0][0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f24f9b7-d064-4948-b378-2aa9ff29709b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "async def generate_concurrently():\n",
    "    llm = OpenAI(temperature = 0.9)\n",
    "    tasks = [async_generate(llm) for _ in range(10)]\n",
    "    await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8195fe01-8475-416a-adb1-42b9e8b4fec5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "I'm doing well, thank you. How about you?\n",
      "\n",
      "\n",
      "I'm doing well, thanks for asking. How about you?\n",
      "\n",
      "\n",
      "I'm doing well, thank you. How about you?\n",
      "\n",
      "\n",
      "I'm doing well, thank you. How about you?\n",
      "\n",
      "\n",
      "I'm doing well, thank you. How about you?\n",
      "\n",
      "\n",
      "I'm doing well, thank you. How about you?\n",
      "\n",
      "\n",
      "I'm doing well, thank you! How about yourself?\n",
      "\n",
      "\n",
      "I'm doing well, thank you. How about yourself?\n",
      "\n",
      "\n",
      "I'm doing well, thanks for asking. How about you?\n",
      "\n",
      "\n",
      "I'm doing well, thank you. How about you?\n",
      "\u001b[1mConcurrent executed in 4.58 seconds.\u001b[0m\n",
      "\n",
      "\n",
      "I'm doing well. How about you?\n",
      "\n",
      "\n",
      "I'm doing well, thank you. How about you?\n",
      "\n",
      "\n",
      "I'm doing well, thank you. How about you?\n",
      "\n",
      "I'm doing well, thank you. How about you?\n",
      "\n",
      "\n",
      "I'm doing well, thank you. How about you?\n",
      "\n",
      "\n",
      "I'm doing well, thanks. How about you?\n",
      "\n",
      "\n",
      "I'm doing well, thank you. How about you?\n",
      "\n",
      "\n",
      "I'm doing well, thank you. How about you?\n",
      "\n",
      "\n",
      "I'm doing well, thank you. How about you?\n",
      "\n",
      "\n",
      "I'm doing well, thank you. How about you?\n",
      "\u001b[1mSerial executed in 10.07 seconds.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "s = time.perf_counter()\n",
    "# If running this outside of Jupyter, use asyncio.run(generate_concurrently())\n",
    "await generate_concurrently()\n",
    "\n",
    "elapsed = time.perf_counter() - s\n",
    "print('\\033[1m' + f\"Concurrent executed in {elapsed:0.2f} seconds.\" + '\\033[0m')\n",
    "\n",
    "s = time.perf_counter()\n",
    "generate_serially()\n",
    "elapsed = time.perf_counter() - s\n",
    "print('\\033[1m' + f\"Serial executed in {elapsed:0.2f} seconds.\" + '\\033[0m')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555b3937-0d60-4e99-87c3-848dbee357bd",
   "metadata": {},
   "source": [
    "### Custom LLM Wrapper\n",
    "For a custom LLM we need to implement -\n",
    "* Required Method: _call \n",
    "* Optional Property: _identifying_params\n",
    "\n",
    "Implementing a simple LLM that returns the first N characters of the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "904425c5-45d5-43b4-860c-fec1727db783",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms.base import LLM\n",
    "from typing import Optional, List, Mapping, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7c4b4f1b-71fa-419c-962b-ea0a339f4193",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomLLM(LLM):\n",
    "    \n",
    "    n: int\n",
    "    \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return 'custom'\n",
    "    \n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        if stop is not None:\n",
    "            raise ValueError(\"stop kwargs are not permitted.\")\n",
    "        return prompt[:self.n]\n",
    "    \n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        \"\"\" Get the identiying parameters. \"\"\"\n",
    "        return {\"n\": self.n,\n",
    "               \"LLM type\": self._llm_type}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2224acf9-c135-4483-8e02-175134d3bc47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = CustomLLM(n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6a19042d-c4d5-4940-b4b2-c3f75fc2284c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a '"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"This is a foobar thing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1bd94718-e879-44a5-baeb-0560f5ca0a7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mCustomLLM\u001b[0m\n",
      "Params: {'n': 10, 'LLM type': 'custom'}\n"
     ]
    }
   ],
   "source": [
    "print(llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc0a1aa-d7f1-4de4-a187-a452cc1fe45e",
   "metadata": {},
   "source": [
    "### Fake LLMs\n",
    "Fake LLM class are used for testing. This mimicks the real LLMs and simulate what would happen when a call goes to a real LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "105d3666-102b-4fb7-9410-ec34593f2a22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms.fake import FakeListLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cd3c3850-2d7b-490f-9927-fb773592b561",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "79a4f88a-f84b-47dd-8f87-3e5be0ce41ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load tools -\n",
    "\n",
    "tools = load_tools([\"python_repl\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "802af34c-0a61-4e55-9143-8ad6fbe457cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Simulating response - \n",
    "\n",
    "responses = [\n",
    "    \"Action: Python REPL\\nAction Input: print(2 + 2)\",\n",
    "    \"Final Answer: 4\"\n",
    "]\n",
    "llm = FakeListLLM(responses = responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "628ff505-acb6-4530-bb0d-5395b5f35d0e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FakeListLLM(cache=None, verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x10e14e580>, responses=['Action: Python REPL\\nAction Input: print(2 + 2)', 'Final Answer: 4'], i=0)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "16a446e0-b4c5-48b8-814e-dd15c40dc05f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize Agent -\n",
    "agent = initialize_agent(tools, llm, agen = AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1f9099b7-0830-4fc3-b13e-c51cb0be41e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAction: Python REPL\n",
      "Action Input: print(2 + 2)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m4\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mFinal Answer: 4\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'4'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the agent -\n",
    "agent.run(\"what is 2 + 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4257508-539b-4470-b385-f44f83987e97",
   "metadata": {},
   "source": [
    "### Cache LLM Calls - In Memory Cache\n",
    "How to cache results of individual LLM calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ed082234-75da-4a53-a770-f03dfdb414d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.cache import InMemoryCache\n",
    "import langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3b049e20-710b-4e32-a56d-c3471bd3f4c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "langchain.llm_cache = InMemoryCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3e991a0b-7ca2-41c3-ac4a-19f6d6205c88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To make the caching really obvious, let's use a slower model.\n",
    "llm = OpenAI(model_name = 'text-davinci-002', n = 2, best_of = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b37a284c-6743-4960-97a3-9a44ae5c2c79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24 ms, sys: 3.63 ms, total: 27.6 ms\n",
      "Wall time: 1.22 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nThe average person farts 14 times a day.'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# this call to the model is not cached\n",
    "llm(\"Tell me a scientic fun facts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "42a5fb07-943c-49c9-ad09-f55b735b4ab7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 214 µs, sys: 3 µs, total: 217 µs\n",
      "Wall time: 227 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nThe average person farts 14 times a day.'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# the second time it is cached and it goes faster\n",
    "llm(\"Tell me a scientic fun facts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457c1e63-8f1b-42f4-844f-c85e987f09e1",
   "metadata": {},
   "source": [
    "### Cache LLM Calls - SQLLite Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6ad52729-af98-427f-9d56-f8a1559cab0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: .langchain.db: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!rm .langchain.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "694922b0-4274-4e07-98fb-4812693408c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.cache import SQLiteCache\n",
    "langchain.llm_cache = SQLiteCache(database_path = \".langchain.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "de2d7ba0-403e-430e-8f56-bc57a43aeab6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 35.8 ms, sys: 12.3 ms, total: 48.1 ms\n",
      "Wall time: 1.71 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nSome scientific fun facts are:\\n\\n-The average person has about 100,000 hairs on their head.\\n\\n-The human heart beats about 100,000 times a day.\\n\\n-The average person breathes about 23,000 times a day.\\n\\n-The average person blinks about 15 times a minute.'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# first time not cached\n",
    "llm(\"Tell me a scientic fun facts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "68ed4606-5098-4449-9b2f-f7967333c1a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.3 ms, sys: 1.3 ms, total: 3.6 ms\n",
      "Wall time: 2.33 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nSome scientific fun facts are:\\n\\n-The average person has about 100,000 hairs on their head.\\n\\n-The human heart beats about 100,000 times a day.\\n\\n-The average person breathes about 23,000 times a day.\\n\\n-The average person blinks about 15 times a minute.'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# it is cached second time onwards\n",
    "llm(\"Tell me a scientic fun facts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6221e7-3d69-40d2-a564-a5f521e63543",
   "metadata": {},
   "source": [
    "### Cache LLM Calls - Redis Cache\n",
    "For that install and start redis server locally\n",
    "\n",
    "```brew services start redis```\n",
    "\n",
    "```brew services info redis```\n",
    "\n",
    "```brew services stop redis```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5a5312ab-cefa-414d-82a1-33901781e87b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from redis import Redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9ec5228c-3e86-4f41-9b1b-dbd48e62f51b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.cache import RedisCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "14676a2b-febf-43ac-bdb7-a412363a30cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "langchain.llm_cache = RedisCache(redis_ = Redis())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fa95c364-2452-4aa3-a401-53c79eb189f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.6 ms, sys: 5.08 ms, total: 29.6 ms\n",
      "Wall time: 2.22 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nThe average person produces about 1.5 liters of gas a day.'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# first time not cached\n",
    "llm(\"Tell me a scientic fun facts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "75c6137e-c484-4f84-8fc8-16b185c45732",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.19 ms, sys: 2.17 ms, total: 4.37 ms\n",
      "Wall time: 3.72 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nThe average person produces about 1.5 liters of gas a day.'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# it is cached second time onwards\n",
    "llm(\"Tell me a scientic fun facts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9659c0d5-e4e7-4629-a46a-7469b6eb5fe6",
   "metadata": {},
   "source": [
    "### Cache LLM Calls - GPTCache (Exact Match Caching)\n",
    "\n",
    "GPTCache is a library for creating semantic cache to store responses from LLM queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0e1d673c-098d-4c4b-9415-065d0eced09c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gptcache\n",
    "from gptcache.processor.pre import get_prompt\n",
    "from gptcache.manager.factory import get_data_manager\n",
    "from langchain.cache import GPTCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "aa00d817-5a4c-4305-ba47-0b9c7654e46c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Avoid multiple caches using the same file, causing different llm model caches to affect each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f8b50301-ec52-49b6-b32d-411d62ebb825",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "file_prefix = \"data_map\"\n",
    "\n",
    "def init_gptcache_map(cache_obj: gptcache.Cache):\n",
    "    global i\n",
    "    cache_path = f'{file_prefix}_{i}.txt'\n",
    "    cache_obj.init(\n",
    "        pre_embedding_func = get_prompt,\n",
    "        data_manager = get_data_manager(data_path = cache_path),\n",
    "    )\n",
    "    i += 1\n",
    "\n",
    "langchain.llm_cache = GPTCache(init_gptcache_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "880496e5-cb08-4dad-9525-16d5b5c9e8ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-27 10:35:50,392 - 8644582208 - adapter.py-adapter:129 - WARNING: failed to save the data to cache, error: _update_cache_callback() got an unexpected keyword argument 'prompt'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.7 ms, sys: 12.6 ms, total: 38.3 ms\n",
      "Wall time: 1.23 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n-The average person farts 14 times a day.\\n-The human brain is about 75% water.\\n-The average person blinks around 15 times a minute.\\n- humans have more than 5 million smell receptors.'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# first time not cached\n",
    "llm(\"Tell me a scientic fun facts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f79ae045-4eed-410f-aa5b-d376e867c260",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-27 10:36:09,102 - 8644582208 - adapter.py-adapter:129 - WARNING: failed to save the data to cache, error: _update_cache_callback() got an unexpected keyword argument 'prompt'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.65 ms, sys: 2.56 ms, total: 12.2 ms\n",
      "Wall time: 1.15 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nA day on Venus lasts for 243 Earth days.'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# it is cached second time onwards\n",
    "llm(\"Tell me a scientic fun facts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93972862-473a-412e-b1bd-9e59809a7e0c",
   "metadata": {},
   "source": [
    "### Cache LLM Calls - GPTCache (Similarity Match Caching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "28f61630-f06e-48fa-b7b1-629fa660bbf0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gptcache\n",
    "from gptcache.processor.pre import get_prompt\n",
    "from gptcache.manager.factory import get_data_manager\n",
    "from langchain.cache import GPTCache\n",
    "from gptcache.manager import get_data_manager, CacheBase, VectorBase\n",
    "from gptcache import Cache\n",
    "from gptcache.embedding import Onnx\n",
    "from gptcache.similarity_evaluation.distance import SearchDistanceEvaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9b8676c6-2620-404d-9f7d-49d45a90a236",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Avoid multiple caches using the same file, causing different llm model caches to affect each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "fe1436d4-6f32-4d4a-92af-46d62badd40b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "file_prefix = \"data_map\"\n",
    "llm_cache = Cache()\n",
    "\n",
    "def init_gptcache_map(cache_obj: gptcache.Cache):\n",
    "    global i\n",
    "    cache_path = f'{file_prefix}_{i}.txt'\n",
    "    onnx = Onnx()\n",
    "    cache_base = CacheBase('sqlite')\n",
    "    vector_base = VectorBase('faiss', dimension = onnx.dimension)\n",
    "    data_manager = get_data_manager(cache_base, vector_base, max_size = 10, clean_size = 2)\n",
    "    cache_obj.init(\n",
    "        pre_embedding_func = get_prompt,\n",
    "        embedding_func = onnx.to_embeddings,\n",
    "        data_manager = data_manager,\n",
    "        similarity_evaluation = SearchDistanceEvaluation(),\n",
    "    )\n",
    "    i += 1\n",
    "    \n",
    "langchain.llm_cache = GPTCache(init_gptcache_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b82fa699-8672-4ff5-949a-6875ab28aaf5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/suvrobanerjee/suvro_space/code/my_code/global_venv/lib/python3.9/site-packages/gptcache/manager/scalar_data/sql_storage.py:24: SAWarning: This declarative base already contains a class with the same class name and module name as gptcache.manager.scalar_data.sql_storage.QuestionTable, and will be replaced in the string-lookup table.\n",
      "  class QuestionTable(Base):\n",
      "/Users/suvrobanerjee/suvro_space/code/my_code/global_venv/lib/python3.9/site-packages/gptcache/manager/scalar_data/sql_storage.py:44: SAWarning: This declarative base already contains a class with the same class name and module name as gptcache.manager.scalar_data.sql_storage.AnswerTable, and will be replaced in the string-lookup table.\n",
      "  class AnswerTable(Base):\n",
      "/Users/suvrobanerjee/suvro_space/code/my_code/global_venv/lib/python3.9/site-packages/gptcache/manager/scalar_data/sql_storage.py:61: SAWarning: This declarative base already contains a class with the same class name and module name as gptcache.manager.scalar_data.sql_storage.QuestionDepTable, and will be replaced in the string-lookup table.\n",
      "  class QuestionDepTable(Base):\n",
      "2023-04-27 10:50:33,349 - 8644582208 - adapter.py-adapter:129 - WARNING: failed to save the data to cache, error: _update_cache_callback() got an unexpected keyword argument 'prompt'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.08 s, sys: 69.7 ms, total: 1.15 s\n",
      "Wall time: 1.96 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nHow do you catch a cheetah? You tie him to a post!'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# The first time, it is not yet in cache, so it should take longer\n",
    "llm(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "28f3802a-99fb-47f4-bea6-3d13522be54a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-27 10:50:47,633 - 8644582208 - adapter.py-adapter:129 - WARNING: failed to save the data to cache, error: _update_cache_callback() got an unexpected keyword argument 'prompt'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.56 s, sys: 13.7 ms, total: 1.57 s\n",
      "Wall time: 1.19 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# This is an exact match, so it finds it in the cache\n",
    "llm(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637d5bca-b85d-4666-a5c8-868611a52068",
   "metadata": {},
   "source": [
    "### Cache LLM Calls - SQLAlchemy Cache\n",
    "You can use SQLAlchemyCache to cache with any SQL database supported by SQLAlchemy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "17fea9d5-1979-4cbc-890c-0932f2f59a27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from langchain.cache import SQLAlchemyCache\n",
    "# from sqlalchemy import create_engine\n",
    "\n",
    "# engine = create_engine(\"postgresql://postgres:postgres@localhost:5432/postgres\")\n",
    "# langchain.llm_cache = SQLAlchemyCache(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cc1f6c-9675-4916-9945-c3dd5e57396d",
   "metadata": {},
   "source": [
    "### Cache LLM Calls - Custom SQLAlchemy Schemas \n",
    "You can define your own declarative SQLAlchemyCache child class to customize the schema used for caching. For example, to support high-speed fulltext prompt indexing with Postgres, use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "752fe4c4-243d-431b-985b-f17b30009e3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import Column, Integer, String, Computed, Index, Sequence\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy_utils import TSVectorType\n",
    "from langchain.cache import SQLAlchemyCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d9448c6a-cb92-4f92-95b6-4a1debf0807e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Base = declarative_base()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4a5dbf3b-d703-4f92-9c24-1edfec24c056",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class FulltextLLMCache(Base):\n",
    "#     \"\"\" Postgres table for fulltext-indexed LLM Cache \"\"\"\n",
    "    \n",
    "#     __tablename__ = \"llm_cache_fulltext\"\n",
    "#     __table_args__ = {'extend_existing': True} # added : https://stackoverflow.com/questions/27812250/sqlalchemy-inheritance-not-working\n",
    "#     id = Column(Integer, Sequence('cache_id'), primary_key = True)\n",
    "#     prompt = Column(String, nullable = False)\n",
    "#     llm = Column(String, nullable = False)\n",
    "#     idx = Column(String)\n",
    "#     response = Column(String)\n",
    "#     prompt_tsv = Column(TSVectorType(), Computed(\"to_tsvector('english', llm || ' ' || prompt)\", persisted = True))\n",
    "#     __table_args__ = (\n",
    "#         Index(\"idx_fulltext_prompt_tsv\", prompt_tsv, postgresql_using = \"gin\"),\n",
    "#     )\n",
    "    \n",
    "# engine = create_engine(\"postgresql://postgres:postgres@localhost:5432/postgres\")\n",
    "# langchain.llm_cache = SQLAlchemyCache(engine, FulltextLLMCache)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64340edf-573e-4aa0-9307-bcfc7d9cf03d",
   "metadata": {},
   "source": [
    "### Optional Caching -\n",
    "We can turn off caching for specifc LLMs if need be even though the global canching is enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "976c5451-9a42-40c9-8217-3d5dcc182d33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = OpenAI(model_name = \"text-davinci-002\", n = 2, best_of = 2, cache = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "decdfbd3-5827-4563-b9ef-1e69aa8b96d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.12 ms, sys: 2.52 ms, total: 11.6 ms\n",
      "Wall time: 699 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "llm(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "4681831e-f6be-4525-8c06-d6ce437d9fee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.93 ms, sys: 2.22 ms, total: 10.1 ms\n",
      "Wall time: 905 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nA man walks into a bar and asks for a beer. The bartender says \"You\\'re out of luck. We\\'ve been closed for fifteen minutes.\"'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "llm(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79e2a6b-167e-4558-8565-683d603f1d6a",
   "metadata": {},
   "source": [
    "### Optional Caching in Chains\n",
    "\n",
    "We can turn off caching for particular nodes in chains. Let's see this example -\n",
    "\n",
    "Here, we will load a summarizer map-reduce chain. We will cache results for the map-step, but then not freeze it for the combine step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dc8e9b4-4639-4e13-a062-b0f14458a759",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00fa7c7f-243f-4617-ac21-d00ec048f2fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = OpenAI(model_name = \"text-davinci-002\")\n",
    "no_cache_llm = OpenAI(model_name = \"text-davinci-002\", cache = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "205b9fc9-5242-4a33-b1f1-df262ac0fd32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains.mapreduce import MapReduceChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "994b42c1-607d-4f8c-a58d-65e5734a824f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ad7c163-c203-4ed8-b8c7-a33621b4f484",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('../../../sample_data/state_of_the_union.txt') as f:\n",
    "    state_of_the_union = f.read()\n",
    "texts = text_splitter.split_text(state_of_the_union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a5b1833-63b7-4fcb-9358-118c11ecf5b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9b76a6c-cd3a-4999-950e-aabd4539b0dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  \\n\\nLast year COVID-19 kept us apart. This year we are finally together again. \\n\\nTonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \\n\\nWith a duty to one another to the American people to the Constitution. \\n\\nAnd with an unwavering resolve that freedom will always triumph over tyranny. \\n\\nSix days ago, Russia’s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. \\n\\nHe thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. \\n\\nHe met the Ukrainian people. \\n\\nFrom President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world. \\n\\nGroups of citizens blocking tanks with their bodies. Everyone from students to retirees teachers turned soldiers defending their homeland. \\n\\nIn this struggle as President Zelenskyy said in his speech to the European Parliament “Light will win over darkness.” The Ukrainian Ambassador to the United States is here tonight. \\n\\nLet each of us here tonight in this Chamber send an unmistakable signal to Ukraine and to the world. \\n\\nPlease rise if you are able and show that, Yes, we the United States of America stand with the Ukrainian people. \\n\\nThroughout our history we’ve learned this lesson when dictators do not pay a price for their aggression they cause more chaos.   \\n\\nThey keep moving.   \\n\\nAnd the costs and the threats to America and the world keep rising.   \\n\\nThat’s why the NATO Alliance was created to secure peace and stability in Europe after World War 2. \\n\\nThe United States is a member along with 29 other nations. \\n\\nIt matters. American diplomacy matters. American resolve matters. \\n\\nPutin’s latest attack on Ukraine was premeditated and unprovoked. \\n\\nHe rejected repeated efforts at diplomacy. \\n\\nHe thought the West and NATO wouldn’t respond. And he thought he could divide us at home. Putin was wrong. We were ready.  Here is what we did.   \\n\\nWe prepared extensively and carefully. \\n\\nWe spent months building a coalition of other freedom-loving nations from Europe and the Americas to Asia and Africa to confront Putin. \\n\\nI spent countless hours unifying our European allies. We shared with the world in advance what we knew Putin was planning and precisely how he would try to falsely justify his aggression.  \\n\\nWe countered Russia’s lies with truth.   \\n\\nAnd now that he has acted the free world is holding him accountable. \\n\\nAlong with twenty-seven members of the European Union including France, Germany, Italy, as well as countries like the United Kingdom, Canada, Japan, Korea, Australia, New Zealand, and many others, even Switzerland. \\n\\nWe are inflicting pain on Russia and supporting the people of Ukraine. Putin is now isolated from the world more than ever. \\n\\nTogether with our allies –we are right now enforcing powerful economic sanctions. \\n\\nWe are cutting off Russia’s largest banks from the international financial system.  \\n\\nPreventing Russia’s central bank from defending the Russian Ruble making Putin’s $630 Billion “war fund” worthless.   \\n\\nWe are choking off Russia’s access to technology that will sap its economic strength and weaken its military for years to come.  \\n\\nTonight I say to the Russian oligarchs and corrupt leaders who have bilked billions of dollars off this violent regime no more. \\n\\nThe U.S. Department of Justice is assembling a dedicated task force to go after the crimes of Russian oligarchs.  \\n\\nWe are joining with our European allies to find and seize your yachts your luxury apartments your private jets. We are coming for your ill-begotten gains.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40727465-a838-41d9-9495-7bb784f97d83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "docs = [Document(page_content = t) for t in texts[:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d665e5ce-8d58-485e-a5fb-9854c3961a08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32019c70-01b4-4350-ac33-af5b0372b7fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  \\n\\nLast year COVID-19 kept us apart. This year we are finally together again. \\n\\nTonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \\n\\nWith a duty to one another to the American people to the Constitution. \\n\\nAnd with an unwavering resolve that freedom will always triumph over tyranny. \\n\\nSix days ago, Russia’s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. \\n\\nHe thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. \\n\\nHe met the Ukrainian people. \\n\\nFrom President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world. \\n\\nGroups of citizens blocking tanks with their bodies. Everyone from students to retirees teachers turned soldiers defending their homeland. \\n\\nIn this struggle as President Zelenskyy said in his speech to the European Parliament “Light will win over darkness.” The Ukrainian Ambassador to the United States is here tonight. \\n\\nLet each of us here tonight in this Chamber send an unmistakable signal to Ukraine and to the world. \\n\\nPlease rise if you are able and show that, Yes, we the United States of America stand with the Ukrainian people. \\n\\nThroughout our history we’ve learned this lesson when dictators do not pay a price for their aggression they cause more chaos.   \\n\\nThey keep moving.   \\n\\nAnd the costs and the threats to America and the world keep rising.   \\n\\nThat’s why the NATO Alliance was created to secure peace and stability in Europe after World War 2. \\n\\nThe United States is a member along with 29 other nations. \\n\\nIt matters. American diplomacy matters. American resolve matters. \\n\\nPutin’s latest attack on Ukraine was premeditated and unprovoked. \\n\\nHe rejected repeated efforts at diplomacy. \\n\\nHe thought the West and NATO wouldn’t respond. And he thought he could divide us at home. Putin was wrong. We were ready.  Here is what we did.   \\n\\nWe prepared extensively and carefully. \\n\\nWe spent months building a coalition of other freedom-loving nations from Europe and the Americas to Asia and Africa to confront Putin. \\n\\nI spent countless hours unifying our European allies. We shared with the world in advance what we knew Putin was planning and precisely how he would try to falsely justify his aggression.  \\n\\nWe countered Russia’s lies with truth.   \\n\\nAnd now that he has acted the free world is holding him accountable. \\n\\nAlong with twenty-seven members of the European Union including France, Germany, Italy, as well as countries like the United Kingdom, Canada, Japan, Korea, Australia, New Zealand, and many others, even Switzerland. \\n\\nWe are inflicting pain on Russia and supporting the people of Ukraine. Putin is now isolated from the world more than ever. \\n\\nTogether with our allies –we are right now enforcing powerful economic sanctions. \\n\\nWe are cutting off Russia’s largest banks from the international financial system.  \\n\\nPreventing Russia’s central bank from defending the Russian Ruble making Putin’s $630 Billion “war fund” worthless.   \\n\\nWe are choking off Russia’s access to technology that will sap its economic strength and weaken its military for years to come.  \\n\\nTonight I say to the Russian oligarchs and corrupt leaders who have bilked billions of dollars off this violent regime no more. \\n\\nThe U.S. Department of Justice is assembling a dedicated task force to go after the crimes of Russian oligarchs.  \\n\\nWe are joining with our European allies to find and seize your yachts your luxury apartments your private jets. We are coming for your ill-begotten gains.', metadata={})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f42cb97b-1928-42d2-98de-f99992c2293d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c77275f9-80ed-499c-89d1-dff7eaad5969",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain = load_summarize_chain(llm, chain_type = \"map_reduce\", reduce_llm = no_cache_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "293b960c-049e-4674-9a9b-bdbf97818bb0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 241 ms, sys: 29.6 ms, total: 270 ms\n",
      "Wall time: 5.65 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nPresident Biden discusses the recent aggression from Russia and the response from the free world. He explains how America and its allies are working together to hold Russia accountable and outlines the various ways in which they are doing so. Finally, he warns the Russian oligarchs that the U.S. is coming for their ill-gotten gains.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "chain.run(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e574e4-4c54-4925-b17b-c65c6dd6a933",
   "metadata": {},
   "source": [
    "**Note** \n",
    "\n",
    "When we run it again, we see that it runs substantially faster but the final answer is different. This is due to caching at the map steps, but not at the reduce step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32adc05d-ed89-4095-a9b9-646c88f850a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19 ms, sys: 3.92 ms, total: 22.9 ms\n",
      "Wall time: 2.84 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\nPresident Biden discusses the sanctions against Russia and the importance of the NATO alliance. He also addresses the issue of Russian oligarchs and their ill-gotten gains. The United States is joining with its European allies to seize assets from Russians in response to Putin's actions in Ukraine. American airspace will be closed to Russian flights, and military, economic, and humanitarian aid will be given to Ukraine. Putin's aggression will not be tolerated, and the United States is prepared to defend its NATO allies if necessary.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "chain.run(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8414a58a-e70c-4ca4-9b83-cbbe2a7392f4",
   "metadata": {},
   "source": [
    "### Serialize LLM classes\n",
    "\n",
    "Write and read an LLM Configuration to and from disk. This is useful if you want to save the configuration for a given LLM (e.g., the provider, the temperature, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9db6b9f9-4b58-4b41-a5c7-817b85b9d8c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.llms.loading import load_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "656fa357-769b-416b-abd7-4133f8130bc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89befe99-fbdc-4a78-bf94-21d187d20f7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = load_llm(\"llm.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67c84c2d-dec3-4c32-8698-a2b32a3f7ddc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAI(cache=None, verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x10ae73760>, client=<class 'openai.api_resources.completion.Completion'>, model_name='text-davinci-003', temperature=0.7, max_tokens=256, top_p=1.0, frequency_penalty=0.0, presence_penalty=0.0, n=1, best_of=1, model_kwargs={}, openai_api_key=None, openai_api_base=None, openai_organization=None, batch_size=20, request_timeout=None, logit_bias={}, max_retries=6, streaming=False, allowed_special=set(), disallowed_special='all')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "351104a5-e3c0-4bcf-a1f3-640f72a1764e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = load_llm('llm.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db103cfb-59e5-43f4-917b-06458fc4a0df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAI(cache=None, verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x10ae73760>, client=<class 'openai.api_resources.completion.Completion'>, model_name='text-davinci-003', temperature=0.7, max_tokens=256, top_p=1.0, frequency_penalty=0.0, presence_penalty=0.0, n=1, best_of=1, model_kwargs={}, openai_api_key=None, openai_api_base=None, openai_organization=None, batch_size=20, request_timeout=None, logit_bias={}, max_retries=6, streaming=False, allowed_special=set(), disallowed_special='all')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3fb2d14e-5edc-4f9d-8bc2-26c123899e3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4fe05d6a-4716-4740-8d1a-98be754b1583",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# llm.save(\"llm.json\")\n",
    "# llm.save(\"llm.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a854db98-4195-4fee-a956-5a2e2e1a7eff",
   "metadata": {},
   "source": [
    "### Stream LLM and Chat Model Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b0ac89c6-cb4e-40d0-ab0f-6027979a4ad2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI, Anthropic\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks.base import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.schema import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "86fc61bd-5725-4b2d-a3d1-99e04e3ccaf1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The midnight sun is a sight to behold\n",
      "It's beauty is something to behold\n",
      "The sky is lit up in a golden hue\n",
      "The stars twinkle in the night sky too\n",
      "\n",
      "The midnight sun is a sight to see\n",
      "It's a sight that's hard to believe\n",
      "The sky is lit up in a golden hue\n",
      "The stars twinkle in the night sky too\n",
      "\n",
      "The midnight sun is a sight to behold\n",
      "It's beauty is something to behold\n",
      "The sky is lit up in a golden hue\n",
      "The stars twinkle in the night sky too\n",
      "\n",
      "The midnight sun is a sight to behold\n",
      "It's beauty is something to behold\n",
      "The sky is lit up in a golden hue\n",
      "The stars twinkle in the night sky too\n",
      "\n",
      "The midnight sun is a sight to behold\n",
      "It's beauty is something to behold\n",
      "The sky is lit up in a golden hue\n",
      "The stars twinkle in the night sky too\n",
      "\n",
      "The midnight sun is a sight to behold\n",
      "It's beauty is something to behold\n",
      "The sky is lit up in a golden hue\n",
      "The stars twinkle in the night sky too\n",
      "\n",
      "The midnight sun is a sight to behold\n",
      "It's beauty is something to behold\n",
      "The sky is lit up in a"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(streaming = True, callback_manager = CallbackManager([StreamingStdOutCallbackHandler()]), verbose = True, temperature = 0)\n",
    "resp = llm(\"Write me a poem about midnight sun.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "09837f97-70ff-48d4-9872-6e23d7f624ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Q: What did the fish say when it hit the wall?\n",
      "A: Dam!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[Generation(text='\\n\\nQ: What did the fish say when it hit the wall?\\nA: Dam!', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {}, 'model_name': 'text-davinci-003'})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.generate([\"Tell me a joke.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6dc29432-c6ef-4896-a5ad-c90f2c832661",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verse 1:\n",
      "Clear and crisp, refreshing to the taste\n",
      "Bubbles dance, effervescence in the glass\n",
      "A thirst quencher, a healthy choice\n",
      "Sparkling water, my favorite voice\n",
      "\n",
      "Chorus:\n",
      "Sparkling water, oh how you shine\n",
      "A bubbly sensation, so divine\n",
      "No sugar, no calories, just pure delight\n",
      "Sparkling water, my drink of the night\n",
      "\n",
      "Verse 2:\n",
      "A perfect mixer, for any cocktail\n",
      "A splash of lime, or a slice of grapefruit\n",
      "A healthy alternative, to soda pop\n",
      "Sparkling water, never gonna stop\n",
      "\n",
      "Chorus:\n",
      "Sparkling water, oh how you shine\n",
      "A bubbly sensation, so divine\n",
      "No sugar, no calories, just pure delight\n",
      "Sparkling water, my drink of the night\n",
      "\n",
      "Bridge:\n",
      "From the mountains to the sea\n",
      "Sparkling water, you're the key\n",
      "To hydration and a healthy life\n",
      "Sparkling water, you're my delight\n",
      "\n",
      "Chorus:\n",
      "Sparkling water, oh how you shine\n",
      "A bubbly sensation, so divine\n",
      "No sugar, no calories, just pure delight\n",
      "Sparkling water, my drink of the night\n",
      "\n",
      "Outro:\n",
      "Sparkling water, you're the one\n",
      "A refreshing drink, under the sun\n",
      "I'll never tire, of your bubbly taste\n",
      "Sparkling water, you're my favorite place."
     ]
    }
   ],
   "source": [
    "chat = ChatOpenAI(streaming = True, callback_manager = CallbackManager([StreamingStdOutCallbackHandler()]), verbose = True, temperature = 0)\n",
    "resp = chat([HumanMessage(content = \"Write me a sing about sparkling water.\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85eb8b01-8406-4f07-9b8f-2a22ac4c67f1",
   "metadata": {},
   "source": [
    "### How to track token usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f520508-ba9e-4f83-a9c7-0ca7b95a0e3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.callbacks import get_openai_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c76f8b28-cc2d-41e5-87ec-60dd6d40b850",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAI(cache=None, verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x10ae73760>, client=<class 'openai.api_resources.completion.Completion'>, model_name='text-davinci-002', temperature=0.7, max_tokens=256, top_p=1, frequency_penalty=0, presence_penalty=0, n=2, best_of=2, model_kwargs={}, openai_api_key=None, openai_api_base=None, openai_organization=None, batch_size=20, request_timeout=None, logit_bias={}, max_retries=6, streaming=False, allowed_special=set(), disallowed_special='all')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = OpenAI(model_name = \"text-davinci-002\", n = 2, best_of = 2)\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d379e131-0de3-493e-a144-3e22b972cead",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 42\n",
      "\tPrompt Tokens: 4\n",
      "\tCompletion Tokens: 38\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.00084\n"
     ]
    }
   ],
   "source": [
    "with get_openai_callback() as cb:\n",
    "    result = llm(\"tell me a joke\")\n",
    "    print(cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cba52e-252c-4d08-9558-0d834c69b854",
   "metadata": {},
   "source": [
    "Anything inside the context manager will get tracked. Here's an example of using it to track multiple calls in sequenece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "25607e65-a688-44ad-9a3c-ad01819ee3e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n"
     ]
    }
   ],
   "source": [
    "with get_openai_callback() as cb:\n",
    "    result = llm(\"tell me a joke\")\n",
    "    result2 = llm(\"tell me a joke\")\n",
    "    print(cb.total_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c45ccdb-52d7-4e17-9bc4-f2c116486cf4",
   "metadata": {},
   "source": [
    "If a chain or agent with multiple steps in it is used, it will track all those steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8672415a-9660-4cff-a579-9754c12ab54d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9efea692-ad38-420a-adf2-14089a4f0e81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature = 0)\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm = llm)\n",
    "agent = initialize_agent(tools, llm, agent = AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "24595039-bfb6-4749-b04e-0d09384a6320",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out who Sachin Tendulkar's son is and his current age\n",
      "Action: Search\n",
      "Action Input: \"Sachin Tendulkar's son\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mArjun Tendulkar\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to find out Arjun Tendulkar's age\n",
      "Action: Search\n",
      "Action Input: \"Arjun Tendulkar age\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m23 years\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to divide 23 by 3\n",
      "Action: Calculator\n",
      "Action Input: 23/3\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 7.666666666666667\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: Arjun Tendulkar is Sachin Tendulkar's son and his current age is 23 years old. His age divided by 3 is 7.666666666666667.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Total Tokens: 1349\n",
      "Prompt Tokens: 1199\n",
      "Completion Tokens: 150\n",
      "Total Cost (USD): $0.02698\n"
     ]
    }
   ],
   "source": [
    "with get_openai_callback() as cb:\n",
    "    response = agent.run(\"Who is Sachin Tendulkar's son and what is his current age. Divide his age by 3\")\n",
    "    print(f\"Total Tokens: {cb.total_tokens}\")\n",
    "    print(f\"Prompt Tokens: {cb.prompt_tokens}\")\n",
    "    print(f\"Completion Tokens: {cb.completion_tokens}\")\n",
    "    print(f\"Total Cost (USD): ${cb.total_cost}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48b768a-d026-4c2e-a813-679d980f79f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global_venv",
   "language": "python",
   "name": "global_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
