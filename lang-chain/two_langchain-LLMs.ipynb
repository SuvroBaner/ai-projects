{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "540ea6f2-4fd7-4dee-b504-b60067aee34d",
   "metadata": {},
   "source": [
    "## The Models\n",
    "\n",
    "1. **LLMs**\n",
    "2. ChatModels\n",
    "3. Text Embedding Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e919231-e76d-4f2a-b7b4-03c585d501d2",
   "metadata": {},
   "source": [
    "### LLMs  - getting started\n",
    "Large Language Models. It is a standard interface for LLM providers like OpenAI, HuggingFace etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "607ec367-06f1-4f64-997e-bdfe2b9e56da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side!'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name = \"text-ada-001\", n = 2, best_of = 2)\n",
    "\n",
    "llm(\"Tell me a joke with programing anecdote\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2a59956-c777-4c7f-b56c-b3eba4a99b50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# batch of requests -\n",
    "llm_result = llm.generate([\"Tell me a Scientific fact\"] * 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f17023d5-a604-4be7-b165-1edbb5ca0e39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[Generation(text='\\n\\nThe Earth is the only \"edient\" on the planet.', generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text='\\n\\nThe moon has an area of 1,000 square kilometers.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nThe moon has an\\n\\nassorted Slang words for these things meaning different things to different people\\n\\n1. its a place where people go to die\\n2. its a place where people can dream\\n3. its a place where people walk\\n4. its a place where peopleECE (electric chair)', generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text='\\n\\nThe universe is around one trillionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a\\n\\nThe universe is one trillionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of', generation_info={'finish_reason': 'length', 'logprobs': None})], [Generation(text=' about the universe\\n\\nThe universe is about the course of the universe and the existance of intelligent life.', generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text='\\n\\nThe human brain is about 10% the size of a human brain and has about a third the number of colors as a human brain.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nThe moon has an up-to-date on-base scientific team.', generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text='\\n\\nThe average life of a DNA molecule is 5.8 years.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nThe average life of a plant is five to seven years.', generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text='\\n\\nThe average lifespan of a human is about 78 years.', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'completion_tokens': 461, 'total_tokens': 486, 'prompt_tokens': 25}, 'model_name': 'text-ada-001'})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94408711-fdee-4ac9-9505-8b5c7f80de04",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(llm_result.generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4eecea06-c2d9-4175-9ed1-4f3375ac9540",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Generation(text='\\n\\nThe Earth is the only \"edient\" on the planet.', generation_info={'finish_reason': 'stop', 'logprobs': None}),\n",
       "  Generation(text='\\n\\nThe moon has an area of 1,000 square kilometers.', generation_info={'finish_reason': 'stop', 'logprobs': None})],\n",
       " [Generation(text='\\n\\nThe moon has an\\n\\nassorted Slang words for these things meaning different things to different people\\n\\n1. its a place where people go to die\\n2. its a place where people can dream\\n3. its a place where people walk\\n4. its a place where peopleECE (electric chair)', generation_info={'finish_reason': 'stop', 'logprobs': None}),\n",
       "  Generation(text='\\n\\nThe universe is around one trillionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a\\n\\nThe universe is one trillionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of a billionth of', generation_info={'finish_reason': 'length', 'logprobs': None})]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_result.generations[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ee53177-799f-4d35-8d19-6563db8d3100",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'completion_tokens': 461,\n",
       "  'total_tokens': 486,\n",
       "  'prompt_tokens': 25},\n",
       " 'model_name': 'text-ada-001'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_result.llm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a108d816-7000-4a08-ac21-40b14a92af26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.get_num_tokens(\"what a joke\") # make sure you have installed tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b114bfe8-44ee-4dd4-95e8-78a01e6cdde6",
   "metadata": {},
   "source": [
    "### Async API for LLMs\n",
    "It uses asyncio library. This is useful for calling multiple LLMs concurrently, as these calls are network-bound.\n",
    "We can use ```agenerate``` method to call an OpenAI LLM asynchronously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d24fcbf8-5eae-4d75-a47c-e7934a476a95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import asyncio\n",
    "\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8609722b-ab4a-4bd4-bbe4-d75d6f7eedf6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_serially():\n",
    "    llm = OpenAI(temperature = 0.9)\n",
    "    for _ in range(10):\n",
    "        resp = llm.generate([\"Hello, how are you?\"])\n",
    "        print(resp.generations[0][0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0923dceb-9a3b-4c04-ab0d-6d9e1e0b1994",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "async def async_generate(llm):\n",
    "    resp = await llm.agenerate([\"Hello, how are you?\"])\n",
    "    print(resp.generations[0][0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f24f9b7-d064-4948-b378-2aa9ff29709b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "async def generate_concurrently():\n",
    "    llm = OpenAI(temperature = 0.9)\n",
    "    tasks = [async_generate(llm) for _ in range(10)]\n",
    "    await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8195fe01-8475-416a-adb1-42b9e8b4fec5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "I'm doing well, thank you. How about you?\n",
      "\n",
      "\n",
      "I'm doing well, thanks for asking. How about you?\n",
      "\n",
      "\n",
      "I'm doing well, thank you. How about you?\n",
      "\n",
      "\n",
      "I'm doing well, thank you. How about you?\n",
      "\n",
      "\n",
      "I'm doing well, thank you. How about you?\n",
      "\n",
      "\n",
      "I'm doing well, thank you. How about you?\n",
      "\n",
      "\n",
      "I'm doing well, thank you! How about yourself?\n",
      "\n",
      "\n",
      "I'm doing well, thank you. How about yourself?\n",
      "\n",
      "\n",
      "I'm doing well, thanks for asking. How about you?\n",
      "\n",
      "\n",
      "I'm doing well, thank you. How about you?\n",
      "\u001b[1mConcurrent executed in 4.58 seconds.\u001b[0m\n",
      "\n",
      "\n",
      "I'm doing well. How about you?\n",
      "\n",
      "\n",
      "I'm doing well, thank you. How about you?\n",
      "\n",
      "\n",
      "I'm doing well, thank you. How about you?\n",
      "\n",
      "I'm doing well, thank you. How about you?\n",
      "\n",
      "\n",
      "I'm doing well, thank you. How about you?\n",
      "\n",
      "\n",
      "I'm doing well, thanks. How about you?\n",
      "\n",
      "\n",
      "I'm doing well, thank you. How about you?\n",
      "\n",
      "\n",
      "I'm doing well, thank you. How about you?\n",
      "\n",
      "\n",
      "I'm doing well, thank you. How about you?\n",
      "\n",
      "\n",
      "I'm doing well, thank you. How about you?\n",
      "\u001b[1mSerial executed in 10.07 seconds.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "s = time.perf_counter()\n",
    "# If running this outside of Jupyter, use asyncio.run(generate_concurrently())\n",
    "await generate_concurrently()\n",
    "\n",
    "elapsed = time.perf_counter() - s\n",
    "print('\\033[1m' + f\"Concurrent executed in {elapsed:0.2f} seconds.\" + '\\033[0m')\n",
    "\n",
    "s = time.perf_counter()\n",
    "generate_serially()\n",
    "elapsed = time.perf_counter() - s\n",
    "print('\\033[1m' + f\"Serial executed in {elapsed:0.2f} seconds.\" + '\\033[0m')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555b3937-0d60-4e99-87c3-848dbee357bd",
   "metadata": {},
   "source": [
    "### Custom LLM Wrapper\n",
    "For a custom LLM we need to implement -\n",
    "* Required Method: _call \n",
    "* Optional Property: _identifying_params\n",
    "\n",
    "Implementing a simple LLM that returns the first N characters of the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "904425c5-45d5-43b4-860c-fec1727db783",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms.base import LLM\n",
    "from typing import Optional, List, Mapping, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7c4b4f1b-71fa-419c-962b-ea0a339f4193",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomLLM(LLM):\n",
    "    \n",
    "    n: int\n",
    "    \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return 'custom'\n",
    "    \n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        if stop is not None:\n",
    "            raise ValueError(\"stop kwargs are not permitted.\")\n",
    "        return prompt[:self.n]\n",
    "    \n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        \"\"\" Get the identiying parameters. \"\"\"\n",
    "        return {\"n\": self.n,\n",
    "               \"LLM type\": self._llm_type}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2224acf9-c135-4483-8e02-175134d3bc47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = CustomLLM(n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6a19042d-c4d5-4940-b4b2-c3f75fc2284c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a '"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"This is a foobar thing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1bd94718-e879-44a5-baeb-0560f5ca0a7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mCustomLLM\u001b[0m\n",
      "Params: {'n': 10, 'LLM type': 'custom'}\n"
     ]
    }
   ],
   "source": [
    "print(llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc0a1aa-d7f1-4de4-a187-a452cc1fe45e",
   "metadata": {},
   "source": [
    "### Fake LLMs\n",
    "Fake LLM class are used for testing. This mimicks the real LLMs and simulate what would happen when a call goes to a real LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "105d3666-102b-4fb7-9410-ec34593f2a22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms.fake import FakeListLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cd3c3850-2d7b-490f-9927-fb773592b561",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "79a4f88a-f84b-47dd-8f87-3e5be0ce41ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load tools -\n",
    "\n",
    "tools = load_tools([\"python_repl\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "802af34c-0a61-4e55-9143-8ad6fbe457cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Simulating response - \n",
    "\n",
    "responses = [\n",
    "    \"Action: Python REPL\\nAction Input: print(2 + 2)\",\n",
    "    \"Final Answer: 4\"\n",
    "]\n",
    "llm = FakeListLLM(responses = responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "628ff505-acb6-4530-bb0d-5395b5f35d0e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FakeListLLM(cache=None, verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x10e14e580>, responses=['Action: Python REPL\\nAction Input: print(2 + 2)', 'Final Answer: 4'], i=0)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "16a446e0-b4c5-48b8-814e-dd15c40dc05f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize Agent -\n",
    "agent = initialize_agent(tools, llm, agen = AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1f9099b7-0830-4fc3-b13e-c51cb0be41e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAction: Python REPL\n",
      "Action Input: print(2 + 2)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m4\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mFinal Answer: 4\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'4'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the agent -\n",
    "agent.run(\"what is 2 + 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4257508-539b-4470-b385-f44f83987e97",
   "metadata": {},
   "source": [
    "### Cache LLM Calls - In Memory Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ed082234-75da-4a53-a770-f03dfdb414d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.cache import InMemoryCache\n",
    "import langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3b049e20-710b-4e32-a56d-c3471bd3f4c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "langchain.llm_cache = InMemoryCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3e991a0b-7ca2-41c3-ac4a-19f6d6205c88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To make the caching really obvious, let's use a slower model.\n",
    "llm = OpenAI(model_name = 'text-davinci-002', n = 2, best_of = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b37a284c-6743-4960-97a3-9a44ae5c2c79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24 ms, sys: 3.63 ms, total: 27.6 ms\n",
      "Wall time: 1.22 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nThe average person farts 14 times a day.'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# this call to the model is not cached\n",
    "llm(\"Tell me a scientic fun facts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "42a5fb07-943c-49c9-ad09-f55b735b4ab7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 214 µs, sys: 3 µs, total: 217 µs\n",
      "Wall time: 227 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nThe average person farts 14 times a day.'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# the second time it is cached and it goes faster\n",
    "llm(\"Tell me a scientic fun facts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457c1e63-8f1b-42f4-844f-c85e987f09e1",
   "metadata": {},
   "source": [
    "### Cache LLM Calls - SQLLite Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6ad52729-af98-427f-9d56-f8a1559cab0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: .langchain.db: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!rm .langchain.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "694922b0-4274-4e07-98fb-4812693408c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.cache import SQLiteCache\n",
    "langchain.llm_cache = SQLiteCache(database_path = \".langchain.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "de2d7ba0-403e-430e-8f56-bc57a43aeab6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 35.8 ms, sys: 12.3 ms, total: 48.1 ms\n",
      "Wall time: 1.71 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nSome scientific fun facts are:\\n\\n-The average person has about 100,000 hairs on their head.\\n\\n-The human heart beats about 100,000 times a day.\\n\\n-The average person breathes about 23,000 times a day.\\n\\n-The average person blinks about 15 times a minute.'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# first time not cached\n",
    "llm(\"Tell me a scientic fun facts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "68ed4606-5098-4449-9b2f-f7967333c1a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.3 ms, sys: 1.3 ms, total: 3.6 ms\n",
      "Wall time: 2.33 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nSome scientific fun facts are:\\n\\n-The average person has about 100,000 hairs on their head.\\n\\n-The human heart beats about 100,000 times a day.\\n\\n-The average person breathes about 23,000 times a day.\\n\\n-The average person blinks about 15 times a minute.'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# it is cached second time onwards\n",
    "llm(\"Tell me a scientic fun facts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6221e7-3d69-40d2-a564-a5f521e63543",
   "metadata": {},
   "source": [
    "### Cache LLM Calls - Redis Cache\n",
    "For that install and start redis server locally\n",
    "\n",
    "```brew services start redis```\n",
    "\n",
    "```brew services info redis```\n",
    "\n",
    "```brew services stop redis```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5a5312ab-cefa-414d-82a1-33901781e87b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from redis import Redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9ec5228c-3e86-4f41-9b1b-dbd48e62f51b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.cache import RedisCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "14676a2b-febf-43ac-bdb7-a412363a30cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "langchain.llm_cache = RedisCache(redis_ = Redis())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fa95c364-2452-4aa3-a401-53c79eb189f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.6 ms, sys: 5.08 ms, total: 29.6 ms\n",
      "Wall time: 2.22 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nThe average person produces about 1.5 liters of gas a day.'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# first time not cached\n",
    "llm(\"Tell me a scientic fun facts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "75c6137e-c484-4f84-8fc8-16b185c45732",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.19 ms, sys: 2.17 ms, total: 4.37 ms\n",
      "Wall time: 3.72 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nThe average person produces about 1.5 liters of gas a day.'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# it is cached second time onwards\n",
    "llm(\"Tell me a scientic fun facts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9659c0d5-e4e7-4629-a46a-7469b6eb5fe6",
   "metadata": {},
   "source": [
    "### Cache LLM Calls - GPTCache (Exact Matching)\n",
    "\n",
    "GPTCache is a library for creating semantic cache to store responses from LLM queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0e1d673c-098d-4c4b-9415-065d0eced09c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gptcache\n",
    "from gptcache.processor.pre import get_prompt\n",
    "from gptcache.manager.factory import get_data_manager\n",
    "from langchain.cache import GPTCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa00d817-5a4c-4305-ba47-0b9c7654e46c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global_venv",
   "language": "python",
   "name": "global_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
