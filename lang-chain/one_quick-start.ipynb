{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dd67efb-4a22-48d2-af7e-f1ada383cb53",
   "metadata": {},
   "source": [
    "## This notebook has the following topics -\n",
    "1. Use langchain to run an LLM\n",
    "2. Combines LLMs and Prompts in a Chain\n",
    "3. Dynamically calling chains based on user inputs using tool, llm and agent\n",
    "4. Adding state / memory to chains & agents\n",
    "5. Building a chat app using Chat Prompts\n",
    "6. Chat Prompt Templates\n",
    "7. Chains with Chat Models\n",
    "8. Chains with Chat Models using state / memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd33868-2a8e-4ec6-bee2-07995249038c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Building a language model application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73ba0083-e20f-4edf-83f9-8b9330eba5d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "303af879-9271-4236-97bd-54473ef6afdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Socktastic!\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(temperature = 0.9)\n",
    "\n",
    "text = 'What would be a good name for a company that makes colorful socks?'\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388cea48-5d9d-498b-b726-2c5fdd16b2b4",
   "metadata": {},
   "source": [
    "### Chains : Combine LLMs and prompts in a multi-step workflow\n",
    "Convert the above user text into prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b55ed290-9385-46fe-8ff8-38109507f597",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3beb9aac-3d86-4224-94c2-ec385812c837",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature = 0.9)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "        input_variables = [\"product\"],\n",
    "        template = 'What would be a good name for a company that makes {product}?',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "218a86cb-85cc-4c1e-8ae5-b5b7ca697183",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What would be a good name for a company that makes colorful socks?\n"
     ]
    }
   ],
   "source": [
    "print(prompt.format(product = \"colorful socks\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa80135e-2be7-4fb3-9840-e3f2df52db9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "chain = LLMChain(llm = llm, prompt = prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45e5ca51-e976-4d01-8868-c6a7883b59e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nSox in the Box.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"colorful socks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20250e8d-4a57-4594-8fa1-bb3e4205d8e9",
   "metadata": {},
   "source": [
    "### Agents: Dynamically call chains based on User Inputs\n",
    "\n",
    "Agents decide which actions to take and in what order. An action can either be using a tool and observing its output, or returning to the user.\n",
    "\n",
    "* Tool : A function that performs a specific duty e.g. google search, database lookup, other chains etc. Input & Output : Both are string\n",
    "* LLM : The language model powering the agent\n",
    "* Agent : The agent to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41d3d3c5-8590-47ed-b0bf-d94c84e0a645",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Here we will use the SerpAPI for Google search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e1a566c-1103-4296-a1ad-427bc69b126a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e08c4da2-a075-4bbb-bd71-d6bb629ca748",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load llm to control the agent-\n",
    "llm = OpenAI(temperature = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "030cb2dd-2fd0-4ec8-bcb5-9d7f198b454d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load some tools -\n",
    "tools = load_tools(['serpapi', 'llm-math'], llm = llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d51aa2a-6bab-44b1-a528-929f47616988",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
    "agent = initialize_agent(tools, llm, agent = AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "066dd0a9-5618-484e-86f9-9ad590cc992d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out the temperature in Bangalore and some open problems in earth sciences.\n",
      "Action: Search\n",
      "Action Input: \"High temperature in Bangalore yesterday\" and \"Open problems in earth sciences\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m80 °F · 81 °F · 79 °F ...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to convert the temperature to Celcius\n",
      "Action: Calculator\n",
      "Action Input: 80 °F in Celcius\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 26.666666666666668\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: The high temperature in Bangalore yesterday in Celcius was 26.666666666666668. Some open problems in earth sciences include climate change, ocean acidification, and land use change.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The high temperature in Bangalore yesterday in Celcius was 26.666666666666668. Some open problems in earth sciences include climate change, ocean acidification, and land use change.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the agent -\n",
    "agent.run(\"What was the high temperature in Bangalore yesterday in Celcius ? What are some open problems in earth sciences ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaed6516-d5a1-441e-858d-9d80124a738d",
   "metadata": {},
   "source": [
    "### Memory : Add State to Chains & Agents\n",
    "The chains and agents are stateless by default. But you can have a chain or agent with some concept of \"memory\" so that it remembers information about its previous interactions. \n",
    "The simple use case is when designing a chatbot, you need to remember the previos messages as a context like a \"short-term memory\" or sometimes \"long-term memory\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90136e95-eec7-4503-8b3d-ecaa4c85e55b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import OpenAI, ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ee207ef-546a-49d0-bb3a-88a671751e8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi there !\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " Hi there! How can I help you?\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(temperature = 0)\n",
    "conversation = ConversationChain(llm = llm, verbose = True)\n",
    "output = conversation.predict(input  = 'Hi there !')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "347bfa86-afb5-450d-b3cf-1d6628579c82",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there !\n",
      "AI:  Hi there! How can I help you?\n",
      "Human: Just want to have a general converstaion with you. \n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " Sure! What would you like to talk about?\n"
     ]
    }
   ],
   "source": [
    "output = conversation.predict(input = \"Just want to have a general converstaion with you. \")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8fd3a8-f009-46d5-b063-5b7d8a4338c8",
   "metadata": {},
   "source": [
    "### Building a langugae model application : Chat Models\n",
    "The above ConversationChain interface you have input, output as texts. But for a chat application, it can have a better abstaction like \"chat\" schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2634d605-be10-4b00-af72-efb322f993c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "chat = ChatOpenAI(temperature = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a5a58d09-b23d-4f78-a6bf-de97add01db0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='मुझे प्रोग्रामिंग करना पसंद है।', additional_kwargs={})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat([HumanMessage(content = 'Translate this sentence from English to Hindi. I love programming')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4591f9f1-875c-4cfd-bcd0-a567d4851a44",
   "metadata": {},
   "source": [
    "You can pass multiple messages for OpenAI's gpt-3.5-turbo and gpt-4 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d2f24e3-ec20-4866-adc9-3220920fddef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='मुझे भारतीय शास्त्रीय गायन करना पसंद है।', additional_kwargs={})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content = \"You are a helpful assistant that translates English to Hindi.\"),\n",
    "    HumanMessage(content = \"Translate this sentence from English to Hindi. I love singing Indian Classical\"),\n",
    "]\n",
    "\n",
    "chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c73bebd2-d8c5-4ec5-a264-c540a0d0ec77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[ChatGeneration(text='मुझे प्रोग्रामिंग करना पसंद है।', generation_info=None, message=AIMessage(content='मुझे प्रोग्रामिंग करना पसंद है।', additional_kwargs={}))], [ChatGeneration(text='मुझे मशीन लर्निंग से प्यार है।', generation_info=None, message=AIMessage(content='मुझे मशीन लर्निंग से प्यार है।', additional_kwargs={}))]], llm_output={'token_usage': {'prompt_tokens': 73, 'completion_tokens': 62, 'total_tokens': 135}, 'model_name': 'gpt-3.5-turbo'})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_messages = [\n",
    "    [\n",
    "        SystemMessage(content = \"You are a helpful assistant that translates English to Hindi.\"),\n",
    "        HumanMessage(content = \"Translate this sentence from English to Hindi. I love programming.\")\n",
    "    ],\n",
    "    [\n",
    "        SystemMessage(content = \"You are a helpful assistant that translates English to Hindi.\"),\n",
    "        HumanMessage(content = \"Translate this sentence from English to Hindi. I love Machine Learning.\")\n",
    "    ],\n",
    "]\n",
    "\n",
    "result = chat.generate(batch_messages)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9470e6c0-e9d3-4551-8f18-55d4bb06670f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[ChatGeneration(text='मुझे प्रोग्रामिंग करना पसंद है।', generation_info=None, message=AIMessage(content='मुझे प्रोग्रामिंग करना पसंद है।', additional_kwargs={}))],\n",
       " [ChatGeneration(text='मुझे मशीन लर्निंग से प्यार है।', generation_info=None, message=AIMessage(content='मुझे मशीन लर्निंग से प्यार है।', additional_kwargs={}))]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2f4790ff-7ecb-4776-9c16-4376215e58c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'prompt_tokens': 73,\n",
       "  'completion_tokens': 62,\n",
       "  'total_tokens': 135},\n",
       " 'model_name': 'gpt-3.5-turbo'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.llm_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eab3f2a-b5e2-4d83-8242-7e1835af4119",
   "metadata": {},
   "source": [
    "### Chat Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c9564c04-95b8-4436-b127-cb62b7f91838",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='मुझे प्रोग्रामिंग से प्यार है।', additional_kwargs={})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "chat = ChatOpenAI(temperature=0)\n",
    "\n",
    "template=\"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template=\"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "# get a chat completion from the formatted messages\n",
    "chat(chat_prompt.format_prompt(input_language=\"English\", output_language=\"Hindi\", text=\"I love programming.\").to_messages())\n",
    "# -> AIMessage(content=\"J'aime programmer.\", additional_kwargs={})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085a80cf-bbce-4b10-8299-848355a76c57",
   "metadata": {},
   "source": [
    "### Chains with Chat Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cd03e4b1-69e3-4800-8324-73f3fc1cbd97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import LLMChain\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "60ad5c9a-2316-40c6-a8d3-9c4cbe1f9ecd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x1226cf880>, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key=None, openai_organization=None, request_timeout=60, max_retries=6, streaming=False, n=1, max_tokens=None)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = ChatOpenAI(temperature = 0)\n",
    "chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e3d425e0-23f4-4241-93f8-72b350805e31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'मुझे प्रोग्रामिंग से प्यार है।'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "\n",
    "human_template = \"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "chain = LLMChain(llm = chat, prompt = chat_prompt)\n",
    "chain.run(input_language = \"English\", output_language = \"Hindi\", text = \"I love programming.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584c1fd0-1bfc-472b-9d05-934c5952803d",
   "metadata": {},
   "source": [
    "### Agents with Chat Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c68fec36-d401-4900-a341-2ab52eca7083",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9906dee2-58a8-4d66-8877-b71820681015",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the model to control the agent -\n",
    "chat = ChatOpenAI(temperature = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "539b4cde-4a17-478f-8068-806c9a7f3cb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the tools \n",
    "llm = OpenAI(temperature = 0)\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm = llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6e257ebf-a4b6-42fb-8427-98e69d173bad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's initialize an agent with the tools, the language model and the type of agent we want to use -\n",
    "agent = initialize_agent(tools, chat, agent = AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bab22868-3c7a-4d97-a473-d396b5484d25",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to use a search engine to find Olivia Wilde's boyfriend and a calculator to raise his age to the 0.23 power.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Search\",\n",
      "  \"action_input\": \"Olivia Wilde boyfriend\"\n",
      "}\n",
      "```\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mSudeikis and Wilde's relationship ended in November 2020. Wilde was publicly served with court documents regarding child custody while she was presenting Don't Worry Darling at CinemaCon 2022. In January 2021, Wilde began dating singer Harry Styles after meeting during the filming of Don't Worry Darling.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI need to use a search engine to find Harry Styles' current age.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Search\",\n",
      "  \"action_input\": \"Harry Styles age\"\n",
      "}\n",
      "```\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m29 years\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mNow I need to calculate 29 raised to the 0.23 power.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Calculator\",\n",
      "  \"action_input\": \"29^0.23\"\n",
      "}\n",
      "```\n",
      "\n",
      "\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 2.169459462491557\u001b[0m\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-JnI81ckwmqzayKRM3GaHFrYA on requests per min. Limit: 3 / min. Please try again in 20s. Contact support@openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 2.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-JnI81ckwmqzayKRM3GaHFrYA on requests per min. Limit: 3 / min. Please try again in 20s. Contact support@openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mI now know the final answer.\n",
      "Final Answer: 2.169459462491557\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.169459462491557'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's test it out -\n",
    "agent.run(\"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05aea4b-606d-4e42-a3b9-63ecea972f2c",
   "metadata": {},
   "source": [
    "### Memory : Add State to Chains and Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6a845994-4442-4957-a43d-306d8f1549ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate\n",
    ")\n",
    "\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fa17db8f-f8f8-4364-91f4-86a3759c4049",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details \\\n",
    "                                               from its context. If the AI does not know the answer to a question, it truthfully says that it does not know.\"),\n",
    "    MessagesPlaceholder(variable_name = \"history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c0925437-6075-4bae-8d62-d254f2d6af0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature = 0)\n",
    "memory = ConversationBufferMemory(return_messages = True)\n",
    "conversation = ConversationChain(memory = memory, prompt = prompt, llm = llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "33d91889-61ad-438c-bbc9-2b262db914f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input = \"Hi there ! \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0502cfa5-4014-4956-9911-ef9e12b2bf14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"That sounds like fun! I'm always here to chat. What would you like to talk about?\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dcd7b173-6dd1-408e-bc99-30a7f048c112",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sure! I'm an AI language model designed to assist with a variety of tasks, from answering questions to generating text. I'm powered by a large neural network that has been trained on vast amounts of data, allowing me to understand and respond to a wide range of topics. I'm constantly learning and improving, so I can provide more accurate and helpful responses over time. Is there anything specific you'd like to know about me?\""
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Tell me about yourself.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81edf8a3-af71-4fab-bcc7-a3bc144701be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global_venv",
   "language": "python",
   "name": "global_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
